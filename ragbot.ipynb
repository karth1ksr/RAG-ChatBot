{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edbd34d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_core.messages import HumanMessage, AIMessage, trim_messages, BaseMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages.utils import count_tokens_approximately\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c59e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEndpointEmbeddings(\n",
    "    model=\"sentence-transformers/all-MiniLM-L6-v2\", task=\"feature-extraction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f80f1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexState(TypedDict):\n",
    "    pdf_path: str\n",
    "    output_dir: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9813d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector(state1: IndexState) -> FAISS:\n",
    "    loader = PyPDFLoader(state1[\"pdf_path\"])\n",
    "    pdf_file = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "    chunks = text_splitter.split_documents(pdf_file)\n",
    "\n",
    "    db = FAISS.from_documents(documents= chunks, embedding=embeddings)\n",
    "    db.save_local(state1[\"output_dir\"])\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f72efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model(state: MessagesState):\n",
    "    query = state[\"messages\"][-1].content\n",
    "    db = FAISS.load_local(r\"./vectordb\", embeddings=embeddings,\n",
    "                           allow_dangerous_deserialization=True)\n",
    "    docs = db.similarity_search(query=query, k=4)\n",
    "    docs_page_content = \" \".join([d.page_content for d in docs])\n",
    "\n",
    "    \n",
    "    promt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\"You are an helpful assistant. Answer to the questions asked \"\n",
    "        \"based on the given context. Question : {question}, context:{context}.\"\n",
    "        \"If the answer is not in the context just say I don't know!\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "        ])\n",
    "    \n",
    "    prompt = promt_template.invoke({\"question\":query, \n",
    "                                    \"context\": docs_page_content,\n",
    "                                    \"messages\":state[\"messages\"]})\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        base_url=\"https://router.huggingface.co/v1\",\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct:together\",\n",
    "        temperature=0.2, \n",
    "    )\n",
    "    \n",
    "    # print(\"Current conversation history:\")\n",
    "    # for m in state[\"messages\"]:\n",
    "    #     print(type(m).__name__, \":\", m.content)\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "114846f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimmer(state: MessagesState):\n",
    "    # msgs = state[\"messages\"]\n",
    "    # # Flatten: handle accidental nesting like [ [HumanMessage(...), AIMessage(...)] , ... ]\n",
    "    # flat: list[BaseMessage] = []\n",
    "    # for m in msgs:\n",
    "    #     if isinstance(m, list):\n",
    "    #         flat.extend(m)\n",
    "    #     else:\n",
    "    #         flat.append(m)\n",
    "\n",
    "    trim = trim_messages(\n",
    "        state[\"messages\"],\n",
    "        max_tokens = 60,\n",
    "        strategy = \"last\",\n",
    "        token_counter = count_tokens_approximately,\n",
    "        include_system = True,\n",
    "        allow_partial = False,\n",
    "        start_on = \"human\",\n",
    "        end_on = (\"human\", \"tool\"),\n",
    "\n",
    "    )\n",
    "\n",
    "    return {\"messages\": trim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a226b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_pipeline():\n",
    "    workflow1 = StateGraph(state_schema=IndexState)\n",
    "\n",
    "    workflow1.add_node(\"vectorize\", create_vector)\n",
    "    workflow1.add_edge(START, \"vectorize\")\n",
    "    workflow1.add_edge(\"vectorize\", END)\n",
    "\n",
    "    app1 = workflow1.compile()\n",
    "\n",
    "    return app1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51a322fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_pipeline():\n",
    "    workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "    workflow.add_node(\"trimming\", trimmer)\n",
    "    workflow.add_node(\"model\", call_model)\n",
    "\n",
    "    workflow.add_edge(START, \"trimming\")\n",
    "    workflow.add_edge(\"trimming\", \"model\")\n",
    "    workflow.add_edge(\"model\", END)\n",
    "\n",
    "    app = workflow.compile(checkpointer=MemorySaver())\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77835d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexpp = index_pipeline()\n",
    "# vectordb = indexpp.invoke({\"pdf_path\":r\".\\vectordb\", \"output_dir\":r\".\\Chatbot with rag\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "073a77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = chat_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8a41cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\":\"ragchat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcb06ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"The chatbot can perform rag and also store convo history\"\n",
    "\n",
    "# input_messages = [HumanMessage(query)]\n",
    "# output = cp.invoke({\"messages\":input_messages}, config)\n",
    "# output[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0267c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'exit' to quit.\n",
      "\n",
      "Bot: Hi Karthik! How can I assist you today?\n",
      "Bot: "
     ]
    }
   ],
   "source": [
    "print(\"Type 'exit' to quit.\\n\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"You: \")\n",
    "    if query.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    \n",
    "    input_messages = [HumanMessage(query)]\n",
    "    \n",
    "    # Stream response\n",
    "    print(\"Bot:\", end=\" \")\n",
    "    for chunk, meta in cp.stream({\"messages\": input_messages}, config, stream_mode=\"messages\",):\n",
    "        if isinstance(chunk, AIMessage):\n",
    "            print(chunk.content, end=\"\", flush=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f3d6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
